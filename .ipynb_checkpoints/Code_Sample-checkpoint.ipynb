{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Eigenface Code Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walkthrough of how I would approach initial data analysis on a small, labelled face dataset scraped from HotorNot.com. As this project is still very much in the works, I use a fixed, vanilla NN structure. Hyperparameter tuning is still in the works, as is collecting a richer dataset, but this should give a picture of how I approach these problems and what my code tends to look like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is organized by gender into male_images and fem_images arrays of dimension (n x 7396)  (That is, 86x86 pixel images). Each image array has a corresponding (1 x n )  *scores array with 0-10 attractiveness ratings per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tools import load_data\n",
    "\n",
    "[male_images, male_scores, fem_images, fem_scores ] = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll first reshape and grayscale the images, for cleanliness down the line. And as the dataset is small (~ 4000 images total), the different color streams may just add more noise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rgb2gray(rgb):\n",
    "    r, g, b = rgb[:,:,0], rgb[:,:,1], rgb[:,:,2]\n",
    "    gray = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    \n",
    "    return gray"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base images for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tools import image_tour\n",
    "\n",
    "image_tour( fem_images, fem_scores, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"images/original/figure_1.png\">\n",
    "<img src=\"images/original/figure_2.png\">\n",
    "<img src=\"images/original/figure_3.png\">\n",
    "\n",
    "Now the average face, the average \"attractive face\" ( 1 standard deviation or more above the norm) and the average \"unattractive face\" ( 1 standard deviation or more below. The differences should be noticeable, but not incredibly so. Also the vector difference between the two scored means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tools import image_averages\n",
    "\n",
    "image_averages(fem_images, fem_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/means/figure_1.png\">\n",
    "<img src=\"images/means/figure_2.png\">\n",
    "<img src=\"images/means/figure_3.png\">\n",
    "<img src=\"images/means/figure_4.png\">\n",
    "\n",
    "Now, after running PCA on the high scoring and low scoring images, is there any visible separation in the top two principle components? Running this may take a moment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tools import pca_plot\n",
    "\n",
    "pca_plot( fem_images, fem_scores )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pca/two_dim.png\">\n",
    "\n",
    "Of course not. For reference, the high scoring images are the blue squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can also use SVD to take a look at some of the eigenvectors reformatted as eigenfaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tools import pca_tour\n",
    "\n",
    "k=5\n",
    "pca_tour(fem_images, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pca/figure_1.png\">\n",
    "<img src=\"images/pca/figure_2.png\">\n",
    "<img src=\"images/pca/figure_3.png\">\n",
    "<img src=\"images/pca/figure_4.png\">\n",
    "<img src=\"images/pca/figure_5.png\">\n",
    "\n",
    "This is great at showing which features go into (unsupervised) separating the data, but ideally we would have a supervised method of telling us which features determine attractive and unattractive faces. For me, this is where neural nets come in. For any dataset, taking a look at the final hidden layer of a trained vanilla NN can capture these features, and unrolling those weights from the beginning of the network can give a decent continuous (small) and VISIBLE image space to examine.\n",
    "\n",
    "Given the number and size of the images, this code will not run in any convenient length of time.\n",
    "\n",
    "Full code is in github repo, this is just main.py, if you are curious about how the forward/ backprop implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from activation_functions import sigmoid_function, tanh_function, linear_function,\\\n",
    "                 LReLU_function, ReLU_function\n",
    "\n",
    "from NeuralNet import NeuralNetwork\n",
    "from tools import load_data\n",
    "import numpy as np\n",
    "\n",
    "# Load data from Hot_or_Not website scrape\n",
    "male_images, male_scores, fem_images, fem_scores = load_data()\n",
    "image_length = male_images.shape[1]\n",
    "\n",
    "settings = {\n",
    "\n",
    "    # Preset Parameters\n",
    "    \"n_inputs\"              :  image_length,        # Number of input signals\n",
    "    \"n_outputs\"             :  1,                   # Number of output signals from the network\n",
    "    \"n_hidden_layers\"       :  1,                   # Number of hidden layers in the network (0 or 1 for now)\n",
    "    \"n_hiddens\"             :  200,                 # Number of nodes per hidden layer\n",
    "    \"activation_functions\"  :  [ LReLU_function, sigmoid_function ],\t\t# Activation functions by layer\n",
    "\n",
    "    # Optional parameters\n",
    "\n",
    "    \"weights_low\"           : -0.1,     # Lower bound on initial weight range\n",
    "    \"weights_high\"          : 0.1,      # Upper bound on initial weight range\n",
    "    \"save_trained_network\"  : False,    # Save trained weights or not.\n",
    "    \"momentum\"              : 0.9,       # Unimplemented as of 1/7/16\n",
    "\n",
    "    \"batch_size\"            : 1,        # 1 for stochastic gradient descent, 0 for gradient descent\n",
    "}\n",
    "\n",
    "# Initialization\n",
    "network = NeuralNetwork( settings )\n",
    "\n",
    "\n",
    "# Train\n",
    "network.train(              fem_images, fem_scores,     # Trainingset\n",
    "                            ERROR_LIMIT    = 1e-3,         # Acceptable error bounds\n",
    "                            learning_rate  = 1e-5,     # Learning Rate\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, updating the images is relatively easy. After forward propogating your image, accumulating the gradients at each layer gives a small perturbation in the image that can be taken as how much each pixel in the original image influences the final rating. This perturbation will increase the target value (and supposedly the attractiveness of the image). Performing gradient descent keeping the weights fixed and the image variable gives the maximally attractive image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does it work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\tdef alter_image(self, image, label, ERROR_LIMIT = 1e-3):\n",
    "\n",
    "\t\tgrad_list = self.backprop(image, label)\n",
    "\t\tepoch = 0\n",
    "\n",
    "\t\twhile MSE > ERROR_LIMIT:\n",
    "\n",
    "\t\t\tepoch +=1\n",
    "\t\t\tdelta = 1\n",
    "\t\t\tfor i in xrange(0, len(grad_list), -1):\n",
    "\t\t\t\tdelta = np.dot(grad_list[i], delta)\n",
    "\t\t\timage -= np.swapaxes(delta, 0, 1)\n",
    "\n",
    "\t\t\tif epoch%10 == 0:\n",
    "\t\t\t\timg = np.reshape(image, (image_width, image_height))\n",
    "\t\t\t\tplt.imshow(img, cmap=cm.Greys_r)\n",
    "\t\t\t\tplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/figure_1.png\">\n",
    "<img src=\"images/figure_altered.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really. In general it seems to noisily bring the values closer to the median, but that's it. And different learning rates wildly shifts which local minima it approaches. Better tuning across hyperparameters, more training time, more data, and adding dropout or convolution would likely both decrease overfitting and allow for meaningful image alteration. Automatically selecting filters (in rgb) or selecting an image crop to boost image ratings would also be workable, and less prone to fitting the exact model parameters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
